{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Group no. 10\n",
    "### Project members: \n",
    "Veronika Cucorova <cucorova@kth.se> \n",
    "\n",
    "Tim Roelofs <tjtro@kth.se> \n",
    "\n",
    "LÃ©o Vuylsteker <leov@kth.se> \n",
    "\n",
    "### Declaration\n",
    "By submitting this solution, it is hereby declared that all individuals listed above have contributed to the solution, either with code that appear in the final solution below, or with code that has been evaluated and compared to the final solution, but for some reason has been excluded. It is also declared that all project members fully understand all parts of the final solution and can explain it upon request.\n",
    "\n",
    "It is furthermore declared that the code below is a contribution by the project members only, and specifically that no part of the solution has been copied from any other source (except for lecture slides at the course ID2214) and no part of the solution has been provided by someone not listed as project member above.\n",
    "\n",
    "It is furthermore declared that it has been understood that no other library/package than the Python 3 standard library, NumPy, pandas and time may be used in the solution for this assignment.\n",
    "\n",
    "### Instructions\n",
    "All assignments starting with number 1 below are mandatory. Satisfactory solutions\n",
    "will give 1 point (in total). If they in addition are good (all parts work more or less \n",
    "as they should), completed on time (submitted before the deadline in Canvas) and according\n",
    "to the instructions, together with satisfactory solutions of assignments starting with \n",
    "number 2 below, then the assignment will receive 2 points (in total).\n",
    "\n",
    "It is highly recommended that you do not develop the code directly within the notebook\n",
    "but that you copy the comments and test cases to your regular development environment\n",
    "and only when everything works as expected, that you paste your functions into this\n",
    "notebook, do a final testing (all cells should succeed) and submit the whole notebook \n",
    "(a single file) in Canvas (do not forget to fill in your group number and names above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused functions from Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "\n",
    "def create_normalization(df, normalizationtype=\"minmax\"):\n",
    "    #   we need to do the modifications on a deep copy of the dataframe \n",
    "    deep_copy_df = df.copy(deep=True)\n",
    "    if normalizationtype == 'minmax':\n",
    "        #   define the min and max function on a dataframe\n",
    "        function1 = lambda dataframe: dataframe.min()\n",
    "        function2 = lambda dataframe: dataframe.max()\n",
    "    elif normalizationtype == 'zscore':\n",
    "        #   define the mean and std function on a dataframe\n",
    "        function1 = lambda dataframe: dataframe.mean()\n",
    "        function2 = lambda dataframe: dataframe.std()\n",
    "    else:\n",
    "        #   if the keyword is not recognize, we make sure that the programme raises an error to indicate\n",
    "        #   where the problem is\n",
    "        raise ValueError('The normalizationtype variable has not been recognized. Please use either minmax '\n",
    "                         'or zscore.')\n",
    "    #   we remove the CLASS and ID columns, which have special meanings, from the working dataframe\n",
    "    wrk_df = deep_copy_df[deep_copy_df.columns.difference(['CLASS', 'ID'])]\n",
    "    #   we select the columns with contain numeric types\n",
    "    wrk_df = wrk_df.select_dtypes(np.number)\n",
    "    dict = {}\n",
    "    for key in wrk_df.columns:\n",
    "        #   we add the column name to the dictionary keys along with the corresponding normalization\n",
    "        dict[key] = (normalizationtype, function1(wrk_df[key]), function2(wrk_df[key]))\n",
    "        if normalizationtype == 'minmax':\n",
    "            #   we apply the corresponding min-max normalization to the column\n",
    "            deep_copy_df[key] = deep_copy_df[key].map(\n",
    "                lambda x: (x-function1(wrk_df[key]))/(function2(wrk_df[key] - function1(wrk_df[key])))\n",
    "            )\n",
    "        else:\n",
    "            #   we apply the corresponding z-normalization to the column\n",
    "            deep_copy_df[key] = deep_copy_df[key].map(\n",
    "                lambda x: (x-function1(wrk_df[key]))/function2(wrk_df[key])\n",
    "            )\n",
    "    return deep_copy_df, dict\n",
    "\n",
    "\n",
    "def apply_normalization(df, normalization):\n",
    "    #   we need to do the modifications on a deep copy of the dataframe \n",
    "    deep_copy_df = df.copy(deep=True)\n",
    "    for key in normalization.keys():\n",
    "        normalizationtype = normalization[key][0]\n",
    "        if normalizationtype == 'minmax':\n",
    "            min = normalization[key][1]\n",
    "            max = normalization[key][2]\n",
    "            #   we apply the corresponding min-max normalization to the column\n",
    "            deep_copy_df[key] = deep_copy_df[key].map(\n",
    "                lambda x: (x-min)/(max-min)\n",
    "            )\n",
    "        elif normalizationtype == 'zscore':\n",
    "            #   we apply the corresponding z-normalization to the column\n",
    "            mean = normalization[key][1]\n",
    "            std = normalization[key][2]\n",
    "            deep_copy_df[key] = deep_copy_df[key].map(\n",
    "                lambda x: (x-mean)/std\n",
    "            )\n",
    "        else:\n",
    "            #   if the keyword is not recognize, we make sure that the programme raises an error to indicate\n",
    "            #   where the problem is\n",
    "            raise ValueError('The normalizationtype variable has not been recognized. Please use either'\n",
    "                             ' minmax or zscore.')\n",
    "    return deep_copy_df\n",
    "\n",
    "\n",
    "def create_imputation(df):\n",
    "    wrk_df = df.copy()\n",
    "    dict = {}\n",
    "    columns = wrk_df.columns.difference(['CLASS', 'ID'])\n",
    "    for key in columns:\n",
    "        #treat column as numeric\n",
    "        if (wrk_df[key].dtypes == np.float64 or wrk_df[key].dtypes == np.int64):\n",
    "            #first thing that came to my mind on checking if all values are null\n",
    "            if (wrk_df[key].isnull().sum() == len(wrk_df[key])):\n",
    "                repl_val = 0\n",
    "            else:\n",
    "                repl_val = wrk_df[key].mean()          \n",
    "        #treat value as categorical\n",
    "        elif (wrk_df[key].dtypes == np.bool or wrk_df[key].dtypes.name == 'category'):\n",
    "            if (wrk_df[key].isnull().sum() == len(wrk_df[key])):\n",
    "                repl_val = wrk_df[key].categories[0]\n",
    "            else: \n",
    "                repl_val = wrk_df[key].mode()[0]\n",
    "        #treat value as a string\n",
    "        elif (wrk_df[key].dtypes == np.object):\n",
    "            if (wrk_df[key].isnull().sum() == len(wrk_df[key])):\n",
    "                repl_val = \"\"\n",
    "            else: \n",
    "                repl_val = wrk_df[key].mode()[0] #mode can return multiple things\n",
    "        else: \n",
    "            raise ValueError('Unknown column type.')\n",
    "\n",
    "        dict[key] = (repl_val)\n",
    "        wrk_df[key].fillna(repl_val, inplace = True)\n",
    "    return wrk_df, dict\n",
    "\n",
    "\n",
    "def apply_imputation(df, repl_vals):\n",
    "    wrk_df = df.copy()\n",
    "    for key in repl_vals:\n",
    "        wrk_df[key].fillna(repl_vals[key], inplace = True)\n",
    "    return wrk_df\n",
    "\n",
    "\n",
    "def create_bins(df, nobins=10, bintype=\"equal-width\"):\n",
    "    newdf = df.copy()\n",
    "    binning = {}\n",
    "    binfunctions = {'equal-width':pd.cut, 'equal-size':pd.qcut}\n",
    "    for key in newdf.columns:\n",
    "        if newdf[key].dtype == np.number and key not in ['CLASS', 'ID']:\n",
    "            res, bins = binfunctions[bintype](newdf[key], nobins, labels=False, retbins=True, duplicates=\"drop\")\n",
    "            newdf[key] = res\n",
    "            #hint 6 implementation. This part assumes that there are more than 2 bins\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf \n",
    "            \n",
    "            binning[key] = bins\n",
    "            \n",
    "    #Hint 4 implementation\n",
    "    for key in newdf.columns:\n",
    "        newdf[key] = newdf[key].astype('category')\n",
    "    \n",
    "    return newdf, binning\n",
    "\n",
    "\n",
    "def apply_bins(df, binning):\n",
    "    newdf = df.copy()\n",
    "    for key in newdf.columns:\n",
    "        if key in binning:\n",
    "            newdf[key] = pd.cut(df[key],binning[key], labels=False)\n",
    "    \n",
    "    #Hint 3 implementation\n",
    "    for key in newdf.columns:\n",
    "        newdf[key] = newdf[key].astype('category')\n",
    "    \n",
    "    return newdf\n",
    "\n",
    "\n",
    "def split(df, testfraction=0.5):\n",
    "    #   generate a permutation of the row numbers\n",
    "    permutation = np.random.permutation(df.index)\n",
    "    #   compute the index separating the testing rows from the training rows in permutation\n",
    "    sep_index = int(permutation.size*testfraction)\n",
    "    #   return two slices of the initial data set: \n",
    "    #   the first one contains 1-testfraction fraction of the dataframe and correspond to the training set\n",
    "    #   and the other one contains testfraction fraction of the dataframe and correspond to the testing set.\n",
    "    return df.iloc[permutation[sep_index:], :], df.iloc[permutation[:sep_index], :]\n",
    "\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    max_prob = []\n",
    "    max_prob = preds.idxmax(axis=1)\n",
    "    #perform pairwise comparison of predicted with actual labels\n",
    "    compared = np.equal(max_prob, labels)\n",
    "    #return the number of true ones\n",
    "    return(len(compared[compared == True])/len(compared))\n",
    "\n",
    "\n",
    "def create_one_hot(df):\n",
    "    newdf = pd.DataFrame()\n",
    "    one_hot = {}\n",
    "    for key in df.columns:\n",
    "        if df[key].dtype in [np.object] and key not in ['CLASS', 'ID']:\n",
    "            one_hot[key] = df[key].unique()\n",
    "            newtab = pd.get_dummies(df[key], prefix=key)\n",
    "            for i in newtab.columns: #converting to floats\n",
    "                newtab[i] = newtab[i].astype(float)\n",
    "            newdf = pd.concat([newdf, newtab], axis=1)\n",
    "        else:\n",
    "            newdf = pd.concat([newdf, df[key]], axis=1)\n",
    "    \n",
    "    return(newdf, one_hot)\n",
    "\n",
    "\n",
    "def apply_one_hot(df, one_hot):\n",
    "    newdf = pd.DataFrame()\n",
    "    for key in df.columns:\n",
    "        if key in one_hot:\n",
    "            cols = pd.CategoricalDtype(categories=one_hot[key])\n",
    "            newcol = df[key].astype(cols)\n",
    "            newtab = pd.get_dummies(newcol, prefix=key)\n",
    "            for i in newtab.columns: #converting to floats\n",
    "                newtab[i] = newtab[i].astype(float)\n",
    "                \n",
    "            newdf = pd.concat([newdf, newtab], axis=1)\n",
    "        else:\n",
    "            newdf = pd.concat([newdf, df[key]], axis=1)\n",
    "    return(newdf)\n",
    "\n",
    "\n",
    "def folds(df, nofolds=10):\n",
    "    #   generate a permutation of the row numbers\n",
    "    permutation = np.random.permutation(df.index)\n",
    "    #   split the permutation into equal-sized subsets of row numbers\n",
    "    folds_indexes = np.array_split(permutation, nofolds)\n",
    "    #   return slices of the data set according to the previous subsets of row numbers\n",
    "    return [df.iloc[indexes, :] for indexes in folds_indexes]\n",
    "\n",
    "\n",
    "def brier_score(preds, labels):\n",
    "    #create boolean mask for each label and assign it as an array to the dict\n",
    "    data = {l:np.array(labels) == l for l in list(preds.columns)}\n",
    "    labels_df = pd.DataFrame(data)    \n",
    "    wrk_df = (labels_df - preds)**2\n",
    "    sum_ = wrk_df.sum().sum()\n",
    "    n = wrk_df.shape[0]\n",
    "    return(sum_/n)\n",
    "\n",
    "\n",
    "def auc(df, correctlabels):\n",
    "    correctlabels = list(correctlabels)\n",
    "    finauc = 0\n",
    "    freq = {}\n",
    "    aucdict = {}\n",
    "    for c in set(correctlabels):\n",
    "        freq[c] = correctlabels.count(c)\n",
    "        y_true = np.array([1 if x is c else 0 for x in correctlabels])\n",
    "        scores = df[c]\n",
    "        scoredct = {}\n",
    "        poscount = scores[[bool(x) for x in y_true]].value_counts()\n",
    "        negcount = scores[[not bool(x) for x in y_true]].value_counts()\n",
    "        for score in sorted(set(scores), reverse=True):\n",
    "            posscore = poscount[score] if score in poscount else 0\n",
    "            negscore = negcount[score] if score in negcount else 0\n",
    "            scoredct[score] = (posscore, negscore)\n",
    "\n",
    "        tot_tp = sum([scoredct[key][0] for key in scoredct])\n",
    "        tot_fp = sum([scoredct[key][1] for key in scoredct])\n",
    "        auc = 0\n",
    "        cov_tp = 0\n",
    "        for i in scoredct:\n",
    "            tp_i = scoredct[i][0]\n",
    "            fp_i = scoredct[i][1]\n",
    "            if fp_i == 0:\n",
    "                cov_tp += tp_i\n",
    "            elif tp_i == 0:\n",
    "                auc += (cov_tp/tot_tp)*(fp_i/tot_fp)\n",
    "            else:\n",
    "                auc += (cov_tp/tot_tp)*(fp_i/tot_fp) + (tp_i/tot_tp)*(fp_i/tot_fp)/2\n",
    "                cov_tp += tp_i\n",
    "        \n",
    "        aucdict[c] = auc       \n",
    "    for key in aucdict:\n",
    "        finauc += freq[key]/len(correctlabels)*aucdict[key]\n",
    "    return finauc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionTree with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "# self.model should be a decision tree (for details, see lecture slides), where the leafs return class probabilities\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: First find the available features (excluding \"CLASS\" and \"ID\"), then find the class counts, e.g., using \n",
    "#         groupby, and calculate the default class probabilities (relative frequencies of the class labels)\n",
    "#\n",
    "# Hint 2: Define a function, e.g., called divide_and_conquer, that takes the above as input together with df \n",
    "#         and min_samples_split, and also a nodeno (starting with 0) to keep track of the generated nodes in the tree\n",
    "#\n",
    "# Hint 3: You may represent the tree under construction as a list of nodes (tuples), on the form:\n",
    "#         (nodeno,\"leaf\",class_probabilities): corresponding to a leaf node where class_probabilities is a vector\n",
    "#                                              with the relative class frequencies (ordered according to self.labels)\n",
    "#         (nodeno,feature,node_dict): corresponding to an internal (non-leaf) node where node_dict is a mapping from\n",
    "#                                     the possible values of feature to child nodes (their nodenos)\n",
    "#\n",
    "# Hint 4: You may evaluate each feature by a function information_content, which takes the group sizes\n",
    "#         for each possible value of the feature together with the class counts of each group as input\n",
    "#\n",
    "# Hint 5: The best feature found (with lowest residual information content) will be used to split the training\n",
    "#         instances, and each sub-group will be used for generating a sub-tree (recursively by divide_and_conquer,\n",
    "#         see lecture slides for details)\n",
    "#\n",
    "# Hint 6: You may make divide_and_conquer return not only a list of nodes, but also a current_node_no; \n",
    "#         by this, each subsequent call to divide_and_conquer for each subset of instances, i.e. for each feature value, \n",
    "#         could use current_node_no as a starting point.\n",
    "#         If you e.g. make the following call:\n",
    "#\n",
    "#         current_node_no, node_list = divide_and_conquer(current_node_no, ...)\n",
    "#\n",
    "#         then the returned value in current_node_no can be used in the next call to divide_and_conquer.\n",
    "#         Node that node_list will contain an arbitrary number of tuples, each element corresponding to a node together \n",
    "#         with a node number. The first element in the list will have the same number as current_node_no when the call \n",
    "#         was made and the last element will have a number one less than current_node_no when returned, e.g., if there is\n",
    "#         only one (leaf) node in the returned list, then current_node_no will only be incremented by one through the above call.\n",
    "#         \n",
    "# Hint 7: The list of nodes output by divide_and_conquer may finally be converted to an array, where each nodeno in the \n",
    "#         tuples corresponds to an index of the array\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the relative class frequencies in the leaves of the decision tree into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(nodeno,row), which for a test row\n",
    "#         finds a leaf node from which class probabilities are obtained\n",
    "# Hint 3: This sub-function may recursively traverse the tree (represented by an array), starting with the nodeno\n",
    "#         that corresponds to the root\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \n",
    "    def ___init___(self):\n",
    "        self.binning = None\n",
    "        self.imputation = None\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, df, nobins=10, bintype='equal-width', min_samples_split=5):\n",
    "        #   We modify the dataset df in order to fill missing values and only have categorical features\n",
    "        df, self.imputation = create_imputation(df)\n",
    "        df, self.binning = create_bins(df, nobins, bintype) \n",
    "        self.labels = df[\"CLASS\"].astype('category')\n",
    "        \n",
    "        #   We compute the class frequencies of this modified df\n",
    "        freqdict = {}\n",
    "        grouped = df.groupby('CLASS')\n",
    "        for key in self.labels.unique():\n",
    "            try:\n",
    "                freqdict[key] = len(grouped.get_group(key))/len(df)\n",
    "            except:\n",
    "                freqdict[key] = 0.0\n",
    "        self.model = {}\n",
    "        \n",
    "        #   The index of the root is set to 0; we will store it as a global variable to keep track of the most recently\n",
    "        #   created node and avoid index issues in divide_and_conquer\n",
    "        self.nodeno = 0\n",
    "        \n",
    "        #   We call the recursive function divide_and_conquer to create the nodes of our model step by step\n",
    "        self.divide_and_conquer(freqdict, df, min_samples_split, self.nodeno)\n",
    "\n",
    "\n",
    "\n",
    "    def divide_and_conquer(self, relative_frequencies, df, min_samples_split, nodeno):\n",
    "        #   stop criterion nÂ°1 : the current data frame contains less than min_samples_slit training samples \n",
    "        if len(df) <= min_samples_split:\n",
    "            #   the current node is then considered as a leaf\n",
    "            self.model[nodeno] = ('leaf', relative_frequencies)\n",
    "            return\n",
    "        \n",
    "        #   next step: find the subsets which minimize the residual info \n",
    "        #   and the corresponding feature used for splitting\n",
    "        min_info_res = float('inf')\n",
    "        best_subsets = None\n",
    "        feature = None\n",
    "        for c in df.columns:\n",
    "            if c not in [\"ID\", \"CLASS\"]:\n",
    "                #   we do that by trying each feature to do the split\n",
    "                ir, subsets = self.information_content(c, df)\n",
    "                if ir < min_info_res:\n",
    "                    min_info_res = ir\n",
    "                    best_subsets = subsets\n",
    "                    feature = c\n",
    "        \n",
    "        #   stop criterion nÂ°2 : the current data frame does not have any feature left to split data\n",
    "        if feature is None:\n",
    "            #   the current node is then considered as a leaf\n",
    "            self.model[nodeno] = ('leaf', relative_frequencies)\n",
    "            return\n",
    "        \n",
    "        #   now we need to create the corresponding node which is not a leaf\n",
    "        #   this means that we need to split the data frame according to the selected feature\n",
    "        #   and then call recursively the function on each subset \n",
    "        #   (and obviously update the values of df, relative_frequency and nodeno)\n",
    "        \n",
    "        #   In the mean time we will create the dictionary storing the children of the current node\n",
    "        node_dict = {}\n",
    "        values = df[feature].unique()  # unique values of the feature\n",
    "        allpossvalues = len(self.binning[feature])-1\n",
    "        \n",
    "        for value in range(allpossvalues):\n",
    "            self.nodeno += 1  # we are adding a new child node => we need to update our global nodeno to avoid conflicts\n",
    "            node_dict[value] = self.nodeno  # the child node will be created at the next available nodeno\n",
    "            if value in values:\n",
    "                subset = best_subsets.get_group(value)  # the subset corresponding to the child node\n",
    "                del subset[feature]  # we already used this feature, by deleting the corresponding column we avoid useless checking later on\n",
    "                #   We compute the class frequencies of this modified df\n",
    "                grouped = subset.groupby('CLASS')\n",
    "                freqdict = {}\n",
    "                for key in self.labels.unique():\n",
    "                    try:\n",
    "                        freqdict[key] = len(grouped.get_group(key))/len(subset)\n",
    "                    except:\n",
    "                        freqdict[key] = 0.0\n",
    "                #   we call the function again on this new child node (Depth-First Search)\n",
    "                self.divide_and_conquer(freqdict, subset, min_samples_split, self.nodeno)\n",
    "            else:\n",
    "                self.model[self.nodeno] = ('leaf', relative_frequencies)\n",
    "        \n",
    "        #   finally, we create the current node with the node_dict which has just been created\n",
    "        self.model[nodeno] = (feature, node_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def information_content(self, feature, I_tot):\n",
    "        feature_values = I_tot[feature].unique()   # unique values of the feature\n",
    "        grouped_I_tot = I_tot.groupby(feature)   # df grouped by feature values\n",
    "        res_info = 0    # residual info of the split according to the feature\n",
    "        for value in feature_values:\n",
    "            I_value = grouped_I_tot.get_group(value)    # subset of I_tot where feature = value\n",
    "            weight = len(I_value)/len(I_tot)  # |I_value| / |I_tot|\n",
    "            classes = I_value['CLASS'].unique()    # unique classes of I_value subset\n",
    "            grouped_I_value = I_value.groupby('CLASS')   # I_value subset grouped by value\n",
    "            info_sum = 0\n",
    "            for _class in classes:\n",
    "                p = len(grouped_I_value.get_group(_class))/len(grouped_I_value)  # P(Class = _class) in I_value subset\n",
    "                info_sum -= p * np.log2(p)\n",
    "            res_info += weight * info_sum\n",
    "            \n",
    "        return res_info, grouped_I_tot\n",
    "    \n",
    "    def predict(self, df):\n",
    "        df = df.drop(columns=['CLASS', 'ID'])\n",
    "        df = apply_imputation(df, self.imputation)\n",
    "        df = apply_bins(df, self.binning)\n",
    "        predictions = pd.DataFrame(columns=self.labels.unique())\n",
    "        for index, row in df.iterrows():\n",
    "            outcome = self.make_prediction(0, row)\n",
    "            predictions = pd.concat([predictions, outcome], ignore_index=True)\n",
    "        return(predictions)\n",
    "    \n",
    "    def make_prediction(self, nodeno,row):\n",
    "        if self.model[nodeno][0] == 'leaf':\n",
    "            prediction = pd.DataFrame.from_records([self.model[nodeno][1]])\n",
    "            return prediction\n",
    "        else:\n",
    "            newnode = self.model[nodeno][1][row[self.model[nodeno][0]]]\n",
    "            prediction = self.make_prediction(newnode, row)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (5, 'equal-width', 3): 2.47 s.\n",
      "Testing time (5, 'equal-width', 3): 0.20 s.\n",
      "Training time (5, 'equal-width', 5): 2.04 s.\n",
      "Testing time (5, 'equal-width', 5): 0.15 s.\n",
      "Training time (5, 'equal-width', 10): 1.51 s.\n",
      "Testing time (5, 'equal-width', 10): 0.15 s.\n",
      "Training time (5, 'equal-size', 3): 3.03 s.\n",
      "Testing time (5, 'equal-size', 3): 0.15 s.\n",
      "Training time (5, 'equal-size', 5): 2.45 s.\n",
      "Testing time (5, 'equal-size', 5): 0.14 s.\n",
      "Training time (5, 'equal-size', 10): 1.93 s.\n",
      "Testing time (5, 'equal-size', 10): 0.14 s.\n",
      "Training time (10, 'equal-width', 3): 3.76 s.\n",
      "Testing time (10, 'equal-width', 3): 0.15 s.\n",
      "Training time (10, 'equal-width', 5): 2.59 s.\n",
      "Testing time (10, 'equal-width', 5): 0.19 s.\n",
      "Training time (10, 'equal-width', 10): 2.37 s.\n",
      "Testing time (10, 'equal-width', 10): 0.14 s.\n",
      "Training time (10, 'equal-size', 3): 3.81 s.\n",
      "Testing time (10, 'equal-size', 3): 0.14 s.\n",
      "Training time (10, 'equal-size', 5): 3.71 s.\n",
      "Testing time (10, 'equal-size', 5): 0.17 s.\n",
      "Training time (10, 'equal-size', 10): 2.62 s.\n",
      "Testing time (10, 'equal-size', 10): 0.19 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.532710</td>\n",
       "      <td>0.606309</td>\n",
       "      <td>0.751842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>0.585608</td>\n",
       "      <td>0.766284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.623317</td>\n",
       "      <td>0.739990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.645337</td>\n",
       "      <td>0.766617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>0.641879</td>\n",
       "      <td>0.758390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>0.562977</td>\n",
       "      <td>0.789628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.646725</td>\n",
       "      <td>0.770803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.560748</td>\n",
       "      <td>0.641512</td>\n",
       "      <td>0.767954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.570093</td>\n",
       "      <td>0.590358</td>\n",
       "      <td>0.777335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.420561</td>\n",
       "      <td>1.029004</td>\n",
       "      <td>0.610476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.420561</td>\n",
       "      <td>1.024331</td>\n",
       "      <td>0.610628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.457944</td>\n",
       "      <td>0.779814</td>\n",
       "      <td>0.637095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Accuracy  Brier score       AUC\n",
       "5  equal-width 3   0.532710     0.606309  0.751842\n",
       "               5   0.616822     0.585608  0.766284\n",
       "               10  0.588785     0.623317  0.739990\n",
       "   equal-size  3   0.607477     0.645337  0.766617\n",
       "               5   0.607477     0.641879  0.758390\n",
       "               10  0.598131     0.562977  0.789628\n",
       "10 equal-width 3   0.560748     0.646725  0.770803\n",
       "               5   0.560748     0.641512  0.767954\n",
       "               10  0.570093     0.590358  0.777335\n",
       "   equal-size  3   0.420561     1.029004  0.610476\n",
       "               5   0.420561     1.024331  0.610628\n",
       "               10  0.457944     0.779814  0.637095"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "tree_model = DecisionTree()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "min_samples_split_values = [3,5,10]\n",
    "parameters = [(nobins,bintype,min_samples_split) for nobins in nobins_values for bintype in bintype_values \n",
    "              for min_samples_split in min_samples_split_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    tree_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1],min_samples_split=parameters[i][2])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = tree_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values,min_samples_split_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.97\n",
      "AUC on training set: 1.00\n",
      "Brier score on training set: 0.03\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "tree_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = tree_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the class DecisionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionForest with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "# random_features: no. of features to evaluate at each split (default = 2), 0 means all features (no random sampling)\n",
    "# notrees: no. of trees in the forest (default = 10)\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "# self.model should be a random forest (for details, see lecture slides)\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: Redefine divide_and_conquer to take one additional argument; random_features, and instead of\n",
    "#         evaluating all features choose a random subset, e.g., by np.random.choice (without replacement)\n",
    "# Hint 2: Generate each tree in the forest from a bootstrap replicate of df, e.g., by np.random.choice \n",
    "#         (with replacement) from the index values of df.\n",
    "#\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the mean of all relative class frequencies in the leaves of the forest into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(row), which for a test row\n",
    "#         finds all leaf nodes and calculates the average of their class probabilities\n",
    "\n",
    "\n",
    "class DecisionForest:\n",
    "    \n",
    "    def ___init___(self):\n",
    "        self.binning = None\n",
    "        self.imputation = None\n",
    "        self.labels = None\n",
    "        self.model = None\n",
    "    \n",
    "    def fit(self, df, nobins=10, bintype='equal-width', min_samples_split=5, random_features=2, notrees=10):\n",
    "        #   We modify the dataset df in order to fill missing values and only have categorical features\n",
    "        df, self.imputation = create_imputation(df)\n",
    "        df, self.binning = create_bins(df, nobins, bintype) \n",
    "        self.labels = df[\"CLASS\"].astype('category')\n",
    "        \n",
    "        #   We compute the class frequencies of this modified df\n",
    "        freqdict = {}\n",
    "        grouped = df.groupby('CLASS')\n",
    "        for key in self.labels.unique():\n",
    "            try:\n",
    "                freqdict[key] = len(grouped.get_group(key))/len(df)\n",
    "            except:\n",
    "                freqdict[key] = 0.0\n",
    "        self.model = []\n",
    "        \n",
    "        for index in range(notrees):\n",
    "            self.tree = {}\n",
    "            \n",
    "            #   The index of the root is set to 0; we will store it as a global variable to keep track of the most recently\n",
    "            #   created node and avoid index issues in divide_and_conquer\n",
    "            self.nodeno = 0\n",
    "\n",
    "            #   We call the recursive function divide_and_conquer to create the nodes of our model step by step\n",
    "            self.divide_and_conquer(freqdict, df, min_samples_split, self.nodeno, random_features)\n",
    "            self.model.append(self.tree)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def divide_and_conquer(self, relative_frequencies, df, min_samples_split, nodeno, random_features):\n",
    "        #   stop criterion nÂ°1 : the current data frame contains less than min_samples_slit training samples \n",
    "        if len(df) <= min_samples_split:\n",
    "            #   the current node is then considered as a leaf\n",
    "            self.tree[nodeno] = ('leaf', relative_frequencies)\n",
    "            return\n",
    "        \n",
    "        #   next step: find the subsets which minimize the residual info \n",
    "        #   and the corresponding feature used for splitting\n",
    "        min_info_res = float('inf')\n",
    "        best_subsets = None\n",
    "        feature = None\n",
    "        \n",
    "        random_columns = [col for col in df.columns.values if col not in ['CLASS', 'ID']]\n",
    "        np.random.shuffle(random_columns)\n",
    "        if(random_features > 0 and random_features < len(random_columns)-2):\n",
    "            random_columns = random_columns[:random_features]\n",
    "            \n",
    "        for c in random_columns:\n",
    "            #   we do that by trying each feature to do the split\n",
    "            ir, subsets = self.information_content(c, df)\n",
    "            if ir < min_info_res:\n",
    "                min_info_res = ir\n",
    "                best_subsets = subsets\n",
    "                feature = c\n",
    "        \n",
    "        #   stop criterion nÂ°2 : the current data frame does not have any feature left to split data\n",
    "        if feature is None:\n",
    "            #   the current node is then considered as a leaf\n",
    "            self.tree[nodeno] = ('leaf', relative_frequencies)\n",
    "            return\n",
    "        \n",
    "        #   now we need to create the corresponding node which is not a leaf\n",
    "        #   this means that we need to split the data frame according to the selected feature\n",
    "        #   and then call recursively the function on each subset \n",
    "        #   (and obviously update the values of df, relative_frequency and nodeno)\n",
    "        \n",
    "        #   In the mean time we will create the dictionary storing the children of the current node\n",
    "        node_dict = {}\n",
    "        values = df[feature].unique()  # unique values of the feature\n",
    "        allpossvalues = len(self.binning[feature])-1\n",
    "        \n",
    "        for value in range(allpossvalues):\n",
    "            self.nodeno += 1  # we are adding a new child node => we need to update our global nodeno to avoid conflicts\n",
    "            node_dict[value] = self.nodeno  # the child node will be created at the next available nodeno\n",
    "            if value in values:\n",
    "                subset = best_subsets.get_group(value)  # the subset corresponding to the child node\n",
    "                del subset[feature]  # we already used this feature, by deleting the corresponding column we avoid useless checking later on\n",
    "                #   We compute the class frequencies of this modified df\n",
    "                grouped = subset.groupby('CLASS')\n",
    "                freqdict = {}\n",
    "                for key in self.labels.unique():\n",
    "                    try:\n",
    "                        freqdict[key] = len(grouped.get_group(key))/len(subset)\n",
    "                    except:\n",
    "                        freqdict[key] = 0.0\n",
    "                #   we call the function again on this new child node (Depth-First Search)\n",
    "                self.divide_and_conquer(freqdict, subset, min_samples_split, self.nodeno, random_features)\n",
    "            else:\n",
    "                self.tree[self.nodeno] = ('leaf', relative_frequencies)\n",
    "        \n",
    "        #   finally, we create the current node with the node_dict which has just been created\n",
    "        self.tree[nodeno] = (feature, node_dict)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def information_content(self, feature, I_tot):\n",
    "        feature_values = I_tot[feature].unique()   # unique values of the feature\n",
    "        grouped_I_tot = I_tot.groupby(feature)   # df grouped by feature values\n",
    "        res_info = 0    # residual info of the split according to the feature\n",
    "        for value in feature_values:\n",
    "            I_value = grouped_I_tot.get_group(value)    # subset of I_tot where feature = value\n",
    "            weight = len(I_value)/len(I_tot)  # |I_value| / |I_tot|\n",
    "            classes = I_value['CLASS'].unique()    # unique classes of I_value subset\n",
    "            grouped_I_value = I_value.groupby('CLASS')   # I_value subset grouped by value\n",
    "            info_sum = 0\n",
    "            for _class in classes:\n",
    "                p = len(grouped_I_value.get_group(_class))/len(grouped_I_value)  # P(Class = _class) in I_value subset\n",
    "                info_sum -= p * np.log2(p)\n",
    "            res_info += weight * info_sum\n",
    "            \n",
    "        return res_info, grouped_I_tot\n",
    "    \n",
    "    def predict(self, df):\n",
    "        None\n",
    "    \n",
    "    def make_prediction(self, row):\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1, 1): 16.22 s.\n",
      "Testing time (1, 1): 0.00 s.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'idxmax'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b8a9b4dc5b56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglass_test_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Testing time {0}: {1:.2f} s.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n\u001b[0m\u001b[0;32m     25\u001b[0m                   auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-67b148ccfc94>\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(preds, labels)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mmax_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[0mmax_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;31m#perform pairwise comparison of predicted with actual labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[0mcompared\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'idxmax'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "forest_model = DecisionForest()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "min_samples_split_values = [1,2,5]\n",
    "random_features_values = [1,2,5]\n",
    "\n",
    "parameters = [(min_samples_split,random_features) for min_samples_split in min_samples_split_values \n",
    "              for random_features in random_features_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    forest_model.fit(glass_train_df,min_samples_split=parameters[i][0],random_features=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = forest_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([min_samples_split_values,random_features_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.96\n",
      "AUC on training set: 1.00\n",
      "Brier score on training set: 0.12\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "forest_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = forest_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
